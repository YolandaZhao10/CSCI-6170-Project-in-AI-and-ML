{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YolandaZhao10/CSCI-6170-Project-in-AI-and-ML/blob/main/hw1/homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Advanced Objective Function and Use Case"
      ],
      "metadata": {
        "id": "bQWDj97YXnVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.1"
      ],
      "metadata": {
        "id": "bevrfXSEU5j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Assumption\n",
        "Given a binary classification dataset\n",
        "$\\mathscr D =\\{(x_i,y_i)\\}_{i=1}^N,\\quad\n",
        "x_i\\in\\mathbb R^d,\\; y_i\\in\\{0,1\\}$,\n",
        "logistic regression models the conditional probability as\n",
        "$$\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\n",
        "\\quad\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$$\n",
        "\n",
        "Base on probability for $y_i = 1$, we can also get conditional probability for $y_i = 0$.\n",
        "\n",
        "$$\n",
        "p(y_i=0\\mid x_i;w)=1-\\sigma(w^\\top x_i).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Maximum Likelihood Estimation\n",
        "Each label $y_i$ is assumed to follow a Bernoulli distribution. The conditional likelihood can be written compactly as\n",
        "\n",
        "$$\n",
        "p(y_i\\mid x_i;w)\n",
        "=\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "\n",
        "Assuming the samples are i.i.d., the likelihood over the entire dataset is\n",
        "\n",
        "$$\n",
        "\\mathscr{L}(w)\n",
        "=\\prod_{i=1}^N p(y_i\\mid x_i;w)\n",
        "=\\prod_{i=1}^N\n",
        "\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "\n",
        "Then, we can taking the logarithm of the likelihood yields the log-likelihood:\n",
        "\n",
        "$$\n",
        "\\ell(w)\n",
        "=\\log \\mathscr{L}(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "y_i \\log \\sigma(w^\\top x_i)\n",
        "+(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "Maximum Likelihood Estimation seeks the parameter $w$ that maximizes the log-likelihood:\n",
        "\n",
        "$$\n",
        "w_{\\text{MLE}}=\\arg\\max_w \\ell(w).\n",
        "$$\n",
        "\n",
        "\n",
        "## Get Objective Function of Logistic Regression Under MLE\n",
        "Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood. Therefore, the objective function of logistic regression under MLE is\n",
        "\n",
        "$$\n",
        "J_{\\text{MLE}}(w)\n",
        "=-\\ell(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "- y_i \\log \\sigma(w^\\top x_i)\n",
        "-(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "This objective function is known as the **logistic loss** or **binary cross-entropy loss**.\n",
        "\n",
        "## MLE vs. MAP for Logistic Regression\n",
        "\n",
        "\n",
        "**Definition**: Given a dataset$\n",
        "\\mathscr{D}=\\{(x_i,y_i)\\}_{i=1}^N \\quad y_i\\in\\{0,1\\},\n",
        "$ Logistic Regression models $\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\\quad\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}$.\n",
        "\n",
        "### Objective\n",
        "- **MLE:** maximize likelihood $p(\\mathscr{D}\\mid w)$  \n",
        "- **MAP:** maximize posterior $p(w\\mid\\mathscr{D}) \\propto p(\\mathscr{D}\\mid w)p(w)$ -> MAP can be interpreted as **MLE + regularization**.\n",
        "\n",
        "### Regularization\n",
        "- **MLE:** no explicit regularization term  \n",
        "- **MAP:** adds prior-based regularization ($-\\log p(w)$)\n",
        "\n",
        "### When it helps\n",
        "- **MLE:** works well with large datasets  \n",
        "- **MAP:** often better for small datasets / high-dimensional features because priors reduce overfitting\n",
        "\n",
        "\n",
        "### Reference\n",
        "[1] S. Aswani, “IEOR 165 – Lecture 8: Regularization (Maximum A Posteriori Estimation),” University of California, Berkeley. [Online]. Available: https://aswani.ieor.berkeley.edu/teaching/SP16/165/lecture_notes/ieor165_lec8.pdf\n",
        "\n",
        "\n",
        "[2] A. Ng, “CS229 Lecture Notes: Logistic Regression,” Stanford University. [Online]. Available: https://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf\n",
        "\n",
        "[3] X. Fern, “Logistic Regression,” Oregon State University, CS534 Lecture Notes. [Online]. Available: https://web.engr.oregonstate.edu/~xfern/classes/cs534-18/Logistic-Regression-3-updated.pdf\n"
      ],
      "metadata": {
        "id": "H2fN8MT3aQ7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.2"
      ],
      "metadata": {
        "id": "Xt7XmW1PU9oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a machine learning problem you wish to solve using Logistic Regression.\n",
        "As someone interested in weather tasks, I chose to work with weather prediction, a domain where classification models can provide practical value for planning and risk management. In this project, we use the **Weather Dataset (Rattle Package)**, which contains historical daily weather observations collected from multiple locations in Australia. Each record includes meteorological measurements such as temperature, humidity, atmospheric pressure, wind direction/speed, rainfall, and cloud coverage.\n",
        "\n",
        "The goal of this project is to solve a **binary classification** problem: predicting whether it will **rain tomorrow** (`RainTomorrow = Yes/No`) based on weather conditions observed today. To address this, we apply **logistic regression**, which is a suitable model because the prediction target is binary and logistic regression naturally outputs a probability value \\(P(y=1\\mid x)\\) through a sigmoid activation function applied to a linear combination of the input features. This probability can then be compared against a threshold to determine the predicted class. In addition to being computationally efficient, logistic regression also provides interpretable feature coefficients, allowing us to understand which weather factors are most strongly associated with rainfall on the following day.\n",
        "\n",
        "\n",
        "**Goal**: Predict whether it will rain tomorrow using historical weather measurements.\n",
        "\n",
        "**Dataset (X, y)**: Using the Kaggle dataset: **Weather Dataset (Rattle Package)**  (loaded locally as `weatherAUS.csv`). Link to dataset: https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package\n",
        "\n",
        "**Target variable (y)**: The dataset provides a binary label: `RainTomorrow ∈ {Yes, No}` We define: $y_i =\n",
        "\\begin{cases}\n",
        "1, & \\text{if RainTomorrow = Yes}\\\\\n",
        "0, & \\text{if RainTomorrow = No}\n",
        "\\end{cases}$ So: $y = \\texttt{RainTomorrow}$\n",
        "\n",
        "**Input features (X)**\n",
        "For each observation \\(i\\), we define the feature vector \\(x_i\\) using weather-related measurements from the same day.\n",
        "\n",
        "A typical choice of \\(X\\) is: $x_i = [\\text{Location},\\text{MinTemp},\\text{MaxTemp},\\text{Rainfall},\\text{Evaporation},\\text{Sunshine},\n",
        "\\text{WindGustDir},\\text{WindGustSpeed},\\text{WindDir9am},\\text{WindDir3pm},\n",
        "\\text{WindSpeed9am},\\text{WindSpeed3pm},\\text{Humidity9am},\\text{Humidity3pm},\n",
        "\\text{Pressure9am},\\text{Pressure3pm},\\text{Cloud9am},\\text{Cloud3pm},\n",
        "\\text{Temp9am},\\text{Temp3pm},\\text{RainToday}]$ where categorical variables such as `Location`, `WindGustDir`, etc. are encoded using **one-hot encoding**. We exclude `Date` since it is not a physical measurement by itself and is typically replaced by engineered temporal features (month/season) if needed.\n",
        "\n",
        "## Why Logistic Regression is the Best Choice\n",
        "\n",
        "1. `RainTomorrow` is a two-class label (Yes/No), which directly matches Logistic Regression.\n",
        "\n",
        "2. Second, Interpretable coefficients Logistic Regression provides interpretable weights: positive coefficient → increases rain probability\n",
        "& negative coefficient → decreases rain probability. This is valuable for weather-related decision support (e.g., agriculture, transportation).\n",
        "\n",
        "3. This dataset contains tens of thousands of rows. Logistic Regression trains quickly, handles large datasets well, and supports regularization (L1/L2) to reduce overfitting.\n",
        "\n",
        "4. Instead of only predicting a class label, Logistic Regression outputs:\n",
        "$P(\\text{RainTomorrow}=\\text{Yes}\\mid x)$ which is useful for risk-based decisions (e.g., predicting rain with confidence thresholds).\n",
        "\n",
        "## Comparison to Another Linear Classification Model (Linear SVM)\n",
        "### Objective functions\n",
        "- **Logistic Regression**: $\\min_{w,b} \\sum_{i=1}^N \\log\\bigl(1+\\exp(-y_i(w^\\top x_i+b))\\bigr)$\n",
        "- **Linear SVM**: $\\min_{w,b} \\sum_{i=1}^N \\max(0, 1 - y_i(w^\\top x_i + b))$\n",
        "\n",
        "### Output\n",
        "- **Logistic Regression**: Probability \\(P(y=1|x)\\)\n",
        "- **Linear SVM**: Class score (not probability)\n",
        "\n",
        "### Loss\n",
        "- **Logistic Regression**: Log loss\n",
        "- **Linear SVM**: Hinge loss\n",
        "\n",
        "### Interpretability\n",
        "- **Logistic Regression**: High\n",
        "- **Linear SVM**: Medium\n",
        "\n",
        "### Best for\n",
        "- **Logistic Regression**: Probabilistic risk estimation\n",
        "- **Linear SVM**: Maximum-margin separation\n",
        "\n",
        "## Why Logistic Regression is preferred here\n",
        "In weather prediction, **probability outputs** are often necessary (e.g., “70% chance of rain tomorrow”), so Logistic Regression is more suitable than SVM unless extra probability calibration is added.\n",
        "\n",
        "## Reference\n",
        "[1] X. Fern, “Logistic Regression,” Oregon State University, CS534 Lecture Notes. [Online]. Available: https://web.engr.oregonstate.edu/~xfern/classes/cs534-18/Logistic-Regression-3-updated.pdf\n",
        "\n",
        "[2] Cornell University, “CS4780 Lecture Note 06: Logistic Regression,” Dept. of Computer Science. [Online]. Available: https://www.cs.cornell.edu/courses/cs4780/2023fa/lectures/lecturenote06.html\n",
        "\n",
        "[3] A. Ng, “CS229 Lecture Notes: Support Vector Machines,” Stanford University. [Online]. Available: https://cs229.stanford.edu/notes2020spring/cs229-notes2.pdf\n",
        "\n",
        "[4] D. Klein, “CS 180 Lecture Notes: Support Vector Machines,” University of California, Berkeley. [Online]. Available: https://people.eecs.berkeley.edu/~klein/cs180f13/lectures/lec14.pdf\n",
        "\n",
        "[5] M. Collins, “Lecture Notes: Support Vector Machines,” Carnegie Mellon University. [Online]. Available: http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n"
      ],
      "metadata": {
        "id": "WtwTOsQ1JWQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.3"
      ],
      "metadata": {
        "id": "OQXdX_S_YfES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping dataset variables to equation variables\n",
        "\n",
        "In the logistic regression formulation shown in the derivation, the binary classification dataset is written as: $$\\mathscr{D} = \\{(x_i, y_i)\\}_{i=1}^{N}, \\quad x_i \\in \\mathbb{R}^d,\\quad y_i \\in \\{0,1\\}.$$\n",
        "\n",
        "In this dataset:\n",
        "\n",
        "- **$N$** = number of valid daily observations used for training (after filtering missing labels, etc.)\n",
        "- **$x_i$** = feature vector derived from the $i$-th row of `weatherAUS.csv`. After preprocessing (e.g., one-hot encoding categorical variables), each example becomes a vector: $x_i \\in \\mathbb{R}^d$ where $d$ is the total number of numerical features plus the number of one-hot encoded categories.\n",
        "- **$y_i$** = label derived from the $i$-th row, specifically the column `RainTomorrow ∈ {Yes, No}` $y_i =\n",
        "\\begin{cases}\n",
        "1, & \\text{if RainTomorrow = Yes}\\\\\n",
        "0, & \\text{if RainTomorrow = No}\n",
        "\\end{cases}$ Thus, \\(y_i\\in\\{0,1\\}\\) matches the Bernoulli assumption used in the MLE derivation.\n",
        "\n",
        "\n",
        "## Connection to Logistic Regression Probability Model\n",
        "\n",
        "The derivation assumes logistic regression models: $p(y_i=1 \\mid x_i; w) = \\sigma(w^\\top x_i), \\quad \\sigma(z)=\\frac{1}{1+e^{-z}}$.\n",
        "\n",
        "In this dataset, this corresponds to: $p(\\text{RainTomorrow=Yes} \\mid \\text{weather features today})$\n",
        "\n",
        "meaning logistic regression outputs the probability that tomorrow will be rainy based on today's meteorological conditions.\n",
        "\n",
        "The conditional probability for the negative class is: $p(y_i=0 \\mid x_i;w) = 1-\\sigma(w^\\top x_i)$, which corresponds to the probability of `RainTomorrow = No`.\n",
        "\n",
        "## Assumptions Highlighted in the MLE Derivation\n",
        "\n",
        "The MLE derivation in the picture relies on several key assumptions. In this dataset, these assumptions translate into the following:\n",
        "\n",
        "### Assumption 1: Binary labels follow a Bernoulli distribution\n",
        "The derivation states each \\(y_i\\) is Bernoulli:\n",
        "\n",
        "\\[\n",
        "y_i \\sim \\text{Bernoulli}(p_i),\\quad p_i=\\sigma(w^\\top x_i).\n",
        "\\]\n",
        "\n",
        "This is appropriate because `RainTomorrow` is binary and can be represented as \\(0/1\\).\n",
        "\n",
        "### Assumption 2: Conditional independence of labels given features\n",
        "Logistic regression assumes that once we condition on \\(x_i\\), the label \\(y_i\\) depends only on \\(x_i\\) and parameters \\(w\\), not on other training examples.\n",
        "\n",
        "This supports writing the likelihood as a product:\n",
        "\n",
        "\\[\n",
        "\\mathcal{L}(w)=\\prod_{i=1}^{N}p(y_i\\mid x_i; w).\n",
        "\\]\n",
        "\n",
        "### Assumption 3: i.i.d. samples (independent and identically distributed)\n",
        "The derivation explicitly assumes samples are i.i.d.:\n",
        "\n",
        "\\[\n",
        "\\{(x_i,y_i)\\}_{i=1}^N \\text{ are i.i.d.}\n",
        "\\]\n",
        "\n",
        "In practice, this is an approximation for weather data because observations are time-dependent (weather has temporal correlation). However, for a baseline supervised learning model, we treat each row as an independent sample after feature extraction.\n",
        "\n",
        "### Assumption 4: Linear decision boundary in feature space\n",
        "Logistic regression uses a linear score \\(w^\\top x_i\\), which implies a linear boundary:\n",
        "\n",
        "\\[\n",
        "w^\\top x + b = 0.\n",
        "\\]\n",
        "\n",
        "Thus, we assume that a linear combination of meteorological features is sufficient to separate rainy vs. non-rainy outcomes reasonably well.\n",
        "\n",
        "### Assumption 5: Correct preprocessing makes \\(x_i\\in\\mathbb{R}^d\\)\n",
        "The mathematical form requires all features to be numeric. Therefore we assume:\n",
        "\n",
        "- categorical features (e.g., `Location`, wind directions) are converted via one-hot encoding\n",
        "- missing values are either removed or imputed\n",
        "- all features are aligned into a consistent numeric feature vector\n",
        "\n",
        "## Summary\n",
        "\n",
        "In summary, the WeatherAUS dataset fits the logistic regression + MLE framework because:\n",
        "\n",
        "- each observation provides a feature vector \\(x_i\\)\n",
        "- the label `RainTomorrow` is naturally binary and can be mapped to \\(y_i\\in\\{0,1\\}\\)\n",
        "- the Bernoulli likelihood and log-likelihood derivation apply directly\n",
        "- we make standard ML assumptions (i.i.d. samples, conditional independence, linearity), acknowledging that real weather data may mildly violate strict independence due to temporal patterns.\n"
      ],
      "metadata": {
        "id": "aVn7R1zXZH4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Dataset and Advanced EDA"
      ],
      "metadata": {
        "id": "1eLY4m1oUwi1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3k76DL9ZwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "# Replace 'my_folder/my_data.csv' with your file's actual path\n",
        "file_path = '/content/drive/MyDrive/'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Z0vknJ9SXw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0dv813QXm01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SCMkhWe2UxFd"
      }
    }
  ]
}