{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJo8Qp5izoSNBPnCQCMOBj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YolandaZhao10/CSCI-6170-Project-in-AI-and-ML/blob/main/homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Advanced Objective Function and Use Case"
      ],
      "metadata": {
        "id": "bQWDj97YXnVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Assumption\n",
        "Given a binary classification dataset\n",
        "$\\mathscr D =\\{(x_i,y_i)\\}_{i=1}^N,\\quad\n",
        "x_i\\in\\mathbb R^d,\\; y_i\\in\\{0,1\\}$,\n",
        "logistic regression models the conditional probability as\n",
        "$$\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\n",
        "\\quad\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$$\n",
        "\n",
        "Base on probability for $y_i = 1$, we can also get conditional probability for $y_i = 0$.\n",
        "\n",
        "$$\n",
        "p(y_i=0\\mid x_i;w)=1-\\sigma(w^\\top x_i).\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Maximum Likelihood Estimation\n",
        "Each label $y_i$ is assumed to follow a Bernoulli distribution. The conditional likelihood can be written compactly as\n",
        "\n",
        "$$\n",
        "p(y_i\\mid x_i;w)\n",
        "=\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "\n",
        "Assuming the samples are i.i.d., the likelihood over the entire dataset is\n",
        "\n",
        "$$\n",
        "\\mathscr{L}(w)\n",
        "=\\prod_{i=1}^N p(y_i\\mid x_i;w)\n",
        "=\\prod_{i=1}^N\n",
        "\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "\n",
        "Then, we can taking the logarithm of the likelihood yields the log-likelihood:\n",
        "\n",
        "$$\n",
        "\\ell(w)\n",
        "=\\log \\mathscr{L}(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "y_i \\log \\sigma(w^\\top x_i)\n",
        "+(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "Maximum Likelihood Estimation seeks the parameter $w$ that maximizes the log-likelihood:\n",
        "\n",
        "$$\n",
        "w_{\\text{MLE}}=\\arg\\max_w \\ell(w).\n",
        "$$\n",
        "\n",
        "\n",
        "## Get Objective Function of Logistic Regression Under MLE\n",
        "Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood. Therefore, the objective function of logistic regression under MLE is\n",
        "\n",
        "$$\n",
        "J_{\\text{MLE}}(w)\n",
        "=-\\ell(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "- y_i \\log \\sigma(w^\\top x_i)\n",
        "-(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "This objective function is known as the **logistic loss** or **binary cross-entropy loss**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H2fN8MT3aQ7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Dataset and Advanced EDA"
      ],
      "metadata": {
        "id": "TC2puH6jZwz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a machine learning problem you wish to solve using Logistic Regression.\n",
        "As someone interested in weather tasks, I chose to work with weather prediction, a domain where classification models can provide practical value for planning and risk management. In this project, we use the **Weather Dataset (Rattle Package)**, which contains historical daily weather observations collected from multiple locations in Australia. Each record includes meteorological measurements such as temperature, humidity, atmospheric pressure, wind direction/speed, rainfall, and cloud coverage.\n",
        "\n",
        "The goal of this project is to solve a **binary classification** problem: predicting whether it will **rain tomorrow** (`RainTomorrow = Yes/No`) based on weather conditions observed today. To address this, we apply **logistic regression**, which is a suitable model because the prediction target is binary and logistic regression naturally outputs a probability value \\(P(y=1\\mid x)\\) through a sigmoid activation function applied to a linear combination of the input features. This probability can then be compared against a threshold to determine the predicted class. In addition to being computationally efficient, logistic regression also provides interpretable feature coefficients, allowing us to understand which weather factors are most strongly associated with rainfall on the following day.\n",
        "\n",
        "\n",
        "**Goal**: Predict whether it will rain tomorrow using historical weather measurements.\n",
        "\n",
        "**Dataset (X, y)**: Using the Kaggle dataset: **Weather Dataset (Rattle Package)**  (loaded locally as `weatherAUS.csv`). Link to dataset: https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package\n",
        "\n",
        "**Target variable (y)**: The dataset provides a binary label: `RainTomorrow ∈ {Yes, No}` We define: $y_i =\n",
        "\\begin{cases}\n",
        "1, & \\text{if RainTomorrow = Yes}\\\\\n",
        "0, & \\text{if RainTomorrow = No}\n",
        "\\end{cases}$ So: $y = \\texttt{RainTomorrow}$\n",
        "\n",
        "**Input features (X)**\n",
        "For each observation \\(i\\), we define the feature vector \\(x_i\\) using weather-related measurements from the same day.\n",
        "\n",
        "A typical choice of \\(X\\) is: $x_i = [\\text{Location},\\text{MinTemp},\\text{MaxTemp},\\text{Rainfall},\\text{Evaporation},\\text{Sunshine},\n",
        "\\text{WindGustDir},\\text{WindGustSpeed},\\text{WindDir9am},\\text{WindDir3pm},\n",
        "\\text{WindSpeed9am},\\text{WindSpeed3pm},\\text{Humidity9am},\\text{Humidity3pm},\n",
        "\\text{Pressure9am},\\text{Pressure3pm},\\text{Cloud9am},\\text{Cloud3pm},\n",
        "\\text{Temp9am},\\text{Temp3pm},\\text{RainToday}]$ where categorical variables such as `Location`, `WindGustDir`, etc. are encoded using **one-hot encoding**. We exclude `Date` since it is not a physical measurement by itself and is typically replaced by engineered temporal features (month/season) if needed.\n",
        "\n",
        "## Why Logistic Regression is the Best Choice\n",
        "\n",
        "1. `RainTomorrow` is a two-class label (Yes/No), which directly matches Logistic Regression.\n",
        "\n",
        "2. Second, Interpretable coefficients Logistic Regression provides interpretable weights: positive coefficient → increases rain probability\n",
        "& negative coefficient → decreases rain probability. This is valuable for weather-related decision support (e.g., agriculture, transportation).\n",
        "\n",
        "3. This dataset contains tens of thousands of rows. Logistic Regression trains quickly, handles large datasets well, and supports regularization (L1/L2) to reduce overfitting.\n",
        "\n",
        "4. Instead of only predicting a class label, Logistic Regression outputs:\n",
        "$P(\\text{RainTomorrow}=\\text{Yes}\\mid x)$ which is useful for risk-based decisions (e.g., predicting rain with confidence thresholds).\n",
        "\n",
        "## Comparison to Another Linear Classification Model (Linear SVM)\n",
        "\n",
        "| Aspect | Logistic Regression | Linear SVM |\n",
        "|--------------|--------------|--------------|\n",
        "|Objective functions| $\\min_{w,b} \\sum_{i=1}^N \\log\\bigl(1+\\exp(-y_i(w^\\top x_i+b))\\bigr)$ | $\\min_{w,b} \\sum_{i=1}^N \\max(0, 1 - y_i(w^\\top x_i + b))$|\n",
        "| Output | Probability \\(P(y=1|x)\\) | Class score (not probability) |\n",
        "| Loss | Log loss | Hinge loss |\n",
        "| Interpretability | High | Medium |\n",
        "| Best for | Probabilistic risk estimation | Maximum-margin separation |\n",
        "\n",
        "## Why Logistic Regression is preferred here\n",
        "In weather prediction, **probability outputs** are often necessary (e.g., “70% chance of rain tomorrow”), so Logistic Regression is more suitable than SVM unless extra probability calibration is added.\n",
        "\n",
        "**Citation (SVM reference):**  \n",
        "Cortes, C., & Vapnik, V. (1995). *Support-vector networks*. Machine Learning, 20, 273–297.\n"
      ],
      "metadata": {
        "id": "WtwTOsQ1JWQo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3k76DL9ZwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "# Replace 'my_folder/my_data.csv' with your file's actual path\n",
        "file_path = '/content/drive/MyDrive/'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Z0vknJ9SXw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0dv813QXm01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}