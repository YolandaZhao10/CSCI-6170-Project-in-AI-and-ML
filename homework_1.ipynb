{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvuJYpu09xjA0agXRYb90S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YolandaZhao10/CSCI-6170-Project-in-AI-and-ML/blob/main/homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Advanced Objective Function and Use Case"
      ],
      "metadata": {
        "id": "bQWDj97YXnVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derive the objective function for Logistic Regression using Maximum LikelihoodEstimation (MLE)\n",
        "1. Given a binary classification dataset\n",
        "$\\mathscr D =\\{(x_i,y_i)\\}_{i=1}^N,\\quad\n",
        "x_i\\in\\mathbb R^d,\\; y_i\\in\\{0,1\\}$,\n",
        "logistic regression models the conditional probability as\n",
        "$$\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\n",
        "\\quad\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$$\n",
        "Base on probability for $y_i = 1$, we can also get conditional probability for $y_i = 0$.\n",
        "$$\n",
        "p(y_i=0\\mid x_i;w)=1-\\sigma(w^\\top x_i).\n",
        "$$\n",
        "\n",
        "2. Each label $y_i$ is assumed to follow a Bernoulli distribution. The conditional likelihood can be written compactly as\n",
        "$$\n",
        "p(y_i\\mid x_i;w)\n",
        "=\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "Assuming the samples are i.i.d., the likelihood over the entire dataset is\n",
        "$$\n",
        "\\mathscr{L}(w)\n",
        "=\\prod_{i=1}^N p(y_i\\mid x_i;w)\n",
        "=\\prod_{i=1}^N\n",
        "\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "\n",
        "3. We can taking the logarithm of the likelihood yields the log-likelihood:\n",
        "$$\n",
        "\\ell(w)\n",
        "=\\log \\mathscr{L}(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "y_i \\log \\sigma(w^\\top x_i)\n",
        "+(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "4. Maximum Likelihood Estimation seeks the parameter $w$ that maximizes the log-likelihood:\n",
        "$$\n",
        "w_{\\text{MLE}}=\\arg\\max_w \\ell(w).\n",
        "$$\n",
        "\n",
        "\n",
        "5. Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood. Therefore, the objective function of logistic regression under MLE is\n",
        "$$\n",
        "J_{\\text{MLE}}(w)\n",
        "=-\\ell(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "- y_i \\log \\sigma(w^\\top x_i)\n",
        "-(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n"
      ],
      "metadata": {
        "id": "H2fN8MT3aQ7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research on the MAP technique for Logistic Regression\n",
        "\n",
        "### 1) MLE vs. MAP: definitions\n",
        "Given data  $\n",
        "\\mathscr{D}=\\{(x_i,y_i)\\}_{i=1}^N,\\quad y_i\\in\\{0,1\\},\n",
        "$ logistic regression models $\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$\n",
        "\n",
        "**MLE (Maximum Likelihood Estimation)**: $\n",
        "w_{\\text{MLE}}=\\arg\\max_w \\; p(\\mathscr{D}\\mid w).\n",
        "$\n",
        "\n",
        "**MAP (Maximum A Posteriori)** incorporates a prior $p(w)$:\n",
        "$\n",
        "w_{\\text{MAP}}=\\arg\\max_w \\; p(w\\mid\\mathscr{D})\n",
        "=\\arg\\max_w \\; p(\\mathscr{D}\\mid w)\\,p(w).\n",
        "$\n",
        "This follows from Bayes' rule:\n",
        "$\n",
        "p(w\\mid\\mathscr{D})=\\frac{p(\\mathscr{D}\\mid w)\\,p(w)}{p(\\mathscr{D})}.\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "### 2) MAP objective form\n",
        "Take logs:\n",
        "\n",
        "$$\n",
        "\\log p(w\\mid\\mathscr{D})\n",
        "=\\log p(\\mathscr{D}\\mid w)+\\log p(w)+C.\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "w_{\\text{MAP}}\n",
        "=\\arg\\max_w\\Big(\\log p(\\mathscr{D}\\mid w)+\\log p(w)\\Big).\n",
        "$$\n",
        "\n",
        "Equivalently in minimization form:\n",
        "\n",
        "$$\n",
        "w_{\\text{MAP}}\n",
        "=\\arg\\min_w\\Big(-\\log p(\\mathscr{D}\\mid w)-\\log p(w)\\Big).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Logistic regression negative log-likelihood (MLE loss)\n",
        "The logistic regression negative log-likelihood is:\n",
        "\n",
        "$$\n",
        "J_{\\text{MLE}}(w)\n",
        "=-\\log p(\\mathscr{D}\\mid w)\n",
        "=\\sum_{i=1}^N\\Big[\n",
        "- y_i\\log\\sigma(w^\\top x_i)\n",
        "-(1-y_i)\\log(1-\\sigma(w^\\top x_i))\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Key idea: MAP = MLE loss + regularization\n",
        "From the MAP minimization objective:\n",
        "\n",
        "$$\n",
        "J_{\\text{MAP}}(w)=J_{\\text{MLE}}(w)-\\log p(w).\n",
        "$$\n",
        "\n",
        "So MAP differs from MLE by adding a penalty term derived from the prior.\n",
        "\n",
        "---\n",
        "\n",
        "## Common priors and their MAP interpretation\n",
        "\n",
        "### (A) Gaussian prior $\\rightarrow$ L2 regularization\n",
        "Assume a zero-mean Gaussian prior:\n",
        "\n",
        "$$\n",
        "p(w)=\\mathcal{N}(0,\\sigma^2 I).\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "-\\log p(w)=\\frac{1}{2\\sigma^2}\\|w\\|_2^2 + \\text{const}.\n",
        "$$\n",
        "\n",
        "So MAP yields:\n",
        "\n",
        "$$\n",
        "J_{\\text{MAP}}(w)\n",
        "=J_{\\text{MLE}}(w)+\\lambda\\|w\\|_2^2,\n",
        "\\quad \\lambda=\\frac{1}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "This shows:\n",
        "\n",
        "> **L2-regularized logistic regression = MAP with Gaussian prior.**\n",
        "\n",
        "---\n",
        "\n",
        "### (B) Laplace prior $\\rightarrow$ L1 regularization\n",
        "Assume a Laplace prior:\n",
        "\n",
        "$$\n",
        "p(w_j)\\propto \\exp\\left(-\\frac{|w_j|}{b}\\right).\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "-\\log p(w)\\propto \\|w\\|_1.\n",
        "$$\n",
        "\n",
        "So MAP yields:\n",
        "\n",
        "$$\n",
        "J_{\\text{MAP}}(w)\n",
        "=J_{\\text{MLE}}(w)+\\lambda\\|w\\|_1.\n",
        "$$\n",
        "\n",
        "This shows:\n",
        "\n",
        "> **L1-regularized logistic regression = MAP with Laplace prior.**\n",
        "\n",
        "---\n",
        "\n",
        "## MAP vs. MLE: differences (brief)\n",
        "\n",
        "1. **Uses a prior**\n",
        "   - MLE maximizes $p(\\mathscr{D}\\mid w)$\n",
        "   - MAP maximizes $p(\\mathscr{D}\\mid w)\\,p(w)$\n",
        "\n",
        "2. **Regularization interpretation**\n",
        "   - MAP adds $-\\log p(w)$, i.e., a complexity penalty\n",
        "   - Gaussian prior $\\rightarrow$ L2 shrinkage\n",
        "   - Laplace prior $\\rightarrow$ L1 sparsity\n",
        "\n",
        "3. **When MAP helps**\n",
        "   MAP often improves generalization when:\n",
        "   - dataset is small\n",
        "   - number of features is large\n",
        "   - features are noisy or highly correlated\n",
        "\n",
        "As $N\\to\\infty$, the likelihood dominates, and MAP approaches MLE.\n",
        "\n",
        "\n",
        "## References (IEEE)\n",
        "\n",
        "[1] S. Aswani, “IEOR 165 – Lecture 8: Regularization (Maximum A Posteriori Estimation),” University of California, Berkeley. [Online]. Available: https://aswani.ieor.berkeley.edu/teaching/SP16/165/lecture_notes/ieor165_lec8.pdf\n",
        "\n",
        "[2] X. Fern, “Logistic Regression,” Oregon State University, CS534 Lecture Notes. [Online]. Available: https://web.engr.oregonstate.edu/~xfern/classes/cs534-18/Logistic-Regression-3-updated.pdf\n",
        "\n",
        "[3] Cornell University, “CS4780 Lecture Note 06: Logistic Regression,” Dept. of Computer Science. [Online]. Available: https://www.cs.cornell.edu/courses/cs4780/2023fa/lectures/lecturenote06.html\n",
        "\n",
        "[4] S.-I. Lee, H. Lee, P. Abbeel, and A. Y. Ng, “Efficient L1 Regularized Logistic Regression,” in *Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI-06)*, 2006. [Online]. Available: https://cdn.aaai.org/AAAI/2006/AAAI06-064.pdf\n"
      ],
      "metadata": {
        "id": "zMmczKgJfN2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a machine learning problem solve using Logistic Regression"
      ],
      "metadata": {
        "id": "jI_QBkU0e9oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Dataset and Advanced EDA"
      ],
      "metadata": {
        "id": "TC2puH6jZwz1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3k76DL9ZwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "# Replace 'my_folder/my_data.csv' with your file's actual path\n",
        "file_path = '/content/drive/MyDrive/'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Z0vknJ9SXw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0dv813QXm01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}