{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo0Jk0mQ+H6tcRvNr9aeRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YolandaZhao10/CSCI-6170-Project-in-AI-and-ML/blob/main/homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Advanced Objective Function and Use Case"
      ],
      "metadata": {
        "id": "bQWDj97YXnVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derive the objective function for Logistic Regression using Maximum LikelihoodEstimation (MLE)\n",
        "1. Given a binary classification dataset\n",
        "$\\mathscr D =\\{(x_i,y_i)\\}_{i=1}^N,\\quad\n",
        "x_i\\in\\mathbb R^d,\\; y_i\\in\\{0,1\\}$,\n",
        "logistic regression models the conditional probability as\n",
        "$$\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\n",
        "\\quad\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$$\n",
        "Base on probability for $y_i = 1$, we can also get conditional probability for $y_i = 0$.\n",
        "$$\n",
        "p(y_i=0\\mid x_i;w)=1-\\sigma(w^\\top x_i).\n",
        "$$\n",
        "\n",
        "2. Each label $y_i$ is assumed to follow a Bernoulli distribution. The conditional likelihood can be written compactly as\n",
        "$$\n",
        "p(y_i\\mid x_i;w)\n",
        "=\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "Assuming the samples are i.i.d., the likelihood over the entire dataset is\n",
        "$$\n",
        "\\mathscr{L}(w)\n",
        "=\\prod_{i=1}^N p(y_i\\mid x_i;w)\n",
        "=\\prod_{i=1}^N\n",
        "\\sigma(w^\\top x_i)^{y_i}\n",
        "\\left(1-\\sigma(w^\\top x_i)\\right)^{1-y_i}.\n",
        "$$\n",
        "\n",
        "3. We can taking the logarithm of the likelihood yields the log-likelihood:\n",
        "$$\n",
        "\\ell(w)\n",
        "=\\log \\mathscr{L}(w)\n",
        "=\\sum_{i=1}^N\n",
        "\\Big[\n",
        "y_i \\log \\sigma(w^\\top x_i)\n",
        "+(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\n",
        "\\Big].\n",
        "$$\n",
        "\n",
        "4. Maximum Likelihood Estimation seeks the parameter $w$ that maximizes the log-likelihood:\n",
        "$$\n",
        "w_{\\text{MLE}}=\\arg\\max_w \\ell(w).\n",
        "$$\n",
        "\n",
        "\n",
        "5. Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood. Therefore, the objective function of logistic regression under MLE is\n",
        "$$\n",
        "J_{\\text{MLE}}(w)\n",
        "=-\\ell(w)\n",
        "=\\sum_{i=1}^N \\Big[ - y_i \\log \\sigma(w^\\top x_i) -(1-y_i)\\log\\left(1-\\sigma(w^\\top x_i)\\right)\\Big].\n",
        "$$\n"
      ],
      "metadata": {
        "id": "H2fN8MT3aQ7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research on the MAP technique for Logistic Regression\n",
        "\n",
        "### MLE vs. MAP: definitions\n",
        "Given data  $\n",
        "\\mathscr{D}=\\{(x_i,y_i)\\}_{i=1}^N,\\quad y_i\\in\\{0,1\\},\n",
        "$ logistic regression models $\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$\n",
        "\n",
        "**MLE (Maximum Likelihood Estimation)**: $\n",
        "w_{\\text{MLE}}=\\arg\\max_w \\; p(\\mathscr{D}\\mid w).\n",
        "$\n",
        "\n",
        "**MAP (Maximum A Posteriori)** incorporates a prior $p(w)$:\n",
        "$w_{\\text{MAP}}\n",
        "=\\arg\\max_w \\; p(w\\mid\\mathscr{D})\n",
        "=\\arg\\max_w \\; \\frac{p(\\mathscr{D}\\mid w)\\,p(w)}{p(\\mathscr{D})}\n",
        "=\\arg\\max_w \\; p(\\mathscr{D}\\mid w)\\,p(w),$\n",
        "\n",
        "\n",
        "### MLE vs. MAP: objective form\n",
        "**MAP objective form**:\n",
        "$\n",
        "w_{\\text{MAP}}\n",
        "=\\arg\\min_w\\Big(-\\log p(\\mathscr{D}\\mid w)-\\log p(w)\\Big).\n",
        "$\n",
        "\n",
        "**Logistic regression negative log-likelihood (MLE loss)**:\n",
        "$\n",
        "J_{\\text{MLE}}(w)\n",
        "=-\\log p(\\mathscr{D}\\mid w)\n",
        "=\\sum_{i=1}^N\\Big[ - y_i\\log\\sigma(w^\\top x_i) -(1-y_i)\\log(1-\\sigma(w^\\top x_i))\n",
        "\\Big].\n",
        "$\n",
        "\n",
        "\n",
        "### MAP = MLE loss + regularization\n",
        "From the MAP minimization objective:\n",
        "\n",
        "$$\n",
        "J_{\\text{MAP}}(w)=J_{\\text{MLE}}(w)-\\log p(w).\n",
        "$$\n",
        "\n",
        "So MAP differs from MLE by adding a penalty term derived from the prior.\n",
        "\n",
        "\n",
        "### Common priors and their MAP interpretation\n",
        "\n",
        "1. Gaussian prior -> L2 regularization\n",
        "Assume a zero-mean Gaussian prior:$\n",
        "p(w)=\\mathcal{N}(0,\\sigma^2 I).\n",
        "$ Then, we can get $ -\\log p(w)=\\frac{1}{2\\sigma^2}\\|w\\|_2^2 + \\text{const}.\n",
        "$\n",
        "So MAP yields: $$\n",
        "J_{\\text{MAP}}(w)\n",
        "=J_{\\text{MLE}}(w)+\\lambda\\|w\\|_2^2,\n",
        "\\quad \\lambda=\\frac{1}{2\\sigma^2}.\n",
        "$$\n",
        "This shows: **L2-regularized logistic regression = MAP with Gaussian prior.**\n",
        "\n",
        "2. Laplace prior -> L1 regularization\n",
        "Assume a Laplace prior:$\n",
        "p(w_j)\\propto \\exp\\left(-\\frac{|w_j|}{b}\\right).\n",
        "$ Then, we can get $\n",
        "-\\log p(w)\\propto \\|w\\|_1.\n",
        "$ So MAP yields:\n",
        "$$\n",
        "J_{\\text{MAP}}(w)\n",
        "=J_{\\text{MLE}}(w)+\\lambda\\|w\\|_1.\n",
        "$$\n",
        "This shows: **L1-regularized logistic regression = MAP with Laplace prior.**\n",
        "\n",
        "\n",
        "### MAP vs. MLE: differences\n",
        "MAP differs from MLE by incorporating a prior distribution over the model parameters. While MLE estimates parameters by maximizing only the likelihood $p(\\mathscr{D}\\mid w)$, MAP maximizes the posterior $p(w\\mid\\mathscr{D}) \\propto p(\\mathscr{D}\\mid w)p(w)$, combining data fit with prior belief. As a result, MAP is equivalent to minimizing the negative log-likelihood plus a regularization term $-\\log p(w)$. In practice, MAP often generalizes better than MLE in small-data or high-dimensional settings by preventing overly large weights and reducing overfitting.\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "[1] S. Aswani, “IEOR 165 – Lecture 8: Regularization (Maximum A Posteriori Estimation),” University of California, Berkeley. [Online]. Available: https://aswani.ieor.berkeley.edu/teaching/SP16/165/lecture_notes/ieor165_lec8.pdf\n",
        "\n",
        "[2] Cornell University, “CS4780 Lecture Note 06: Logistic Regression,” Dept. of Computer Science. [Online]. Available: https://www.cs.cornell.edu/courses/cs4780/2023fa/lectures/lecturenote06.html\n",
        "\n",
        "[3] S.-I. Lee, H. Lee, P. Abbeel, and A. Y. Ng, “Efficient L1 Regularized Logistic Regression,” in *Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI-06)*, 2006. [Online]. Available: https://cdn.aaai.org/AAAI/2006/AAAI06-064.pdf\n",
        "\n",
        "[4] R. Garnett, “Logistic Regression,” Washington Univ. in St. Louis, CSE 515T Lecture Notes, 2024. [Online]. Available: https://www.cse.wustl.edu/~garnett/cse515t/fall_2024/files/lecture_notes/9.pdf\n",
        "\n",
        "[5] X. Fern, “Logistic Regression,” Oregon State University, CS534 Lecture Notes. [Online]. Available: https://web.engr.oregonstate.edu/~xfern/classes/cs534-18/Logistic-Regression-3-updated.pdf\n",
        "\n",
        "[6] C. M. Bishop and N. M. Nasrabadi, “MAP Estimation,” in *Pattern Recognition and Machine Learning*. Springer, 2006.\n"
      ],
      "metadata": {
        "id": "zMmczKgJfN2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a machine learning problem solve using Logistic Regression\n",
        "### Problem Definition (RPI GPA → Salary Outcome)\n",
        "Given an RPI undergraduate student dataset\n",
        "$\n",
        "\\mathscr{D}=\\{(x_i,y_i)\\}_{i=1}^N,\n",
        "$ each feature vector $x_i\\in\\mathbb{R}^d$ contains student information such as:\n",
        "- overall GPA  \n",
        "- Math department GPA  \n",
        "- major / department  \n",
        "- internship experience (yes/no or count)  \n",
        "- research experience  \n",
        "- graduation year, etc.\n",
        "\n",
        "The target label is defined as a binary variable:\n",
        "$$\n",
        "y_i=\n",
        "\\begin{cases}\n",
        "1, & \\text{if the student’s salary 10 years after graduation } \\ge T,\\\\\n",
        "0, & \\text{otherwise},\n",
        "\\end{cases}\n",
        "$$\n",
        "where $T$ is a chosen salary threshold (e.g., $T=\\$150{,}000$).\n",
        "\n",
        "Logistic Regression models the probability of reaching the high-salary threshold as$\n",
        "p(y_i=1\\mid x_i;w)=\\sigma(w^\\top x_i),\n",
        "\\quad\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
        "$\n",
        "\n",
        "The prediction is then\n",
        "$$\n",
        "\\hat{y}_i=\n",
        "\\begin{cases}\n",
        "1,& \\text{if } p(y_i=1\\mid x_i;w)\\ge 0.5,\\\\\n",
        "0,& \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "### Justification: Why Logistic Regression?\n",
        "\n",
        "Logistic Regression is an appropriate choice for this problem because it is designed for binary classification and directly models the probability of a student reaching a high-salary outcome:\n",
        "$$\n",
        "p(y=1\\mid x;w)=\\sigma(w^\\top x).\n",
        "$$\n",
        "This probabilistic output is useful because it provides not only a class prediction but also a confidence score, which allows flexible decision thresholds (e.g., choosing a stricter threshold for identifying high-salary candidates). In addition, Logistic Regression is interpretable: the learned weights indicate how features such as overall GPA or Math GPA increase or decrease the likelihood of reaching the salary threshold.\n",
        "\n",
        "### Brief Comparison to Another Linear Model (Linear SVM)\n",
        "\n",
        "A common alternative linear classification model is the Linear Support Vector Machine (SVM). Both Logistic Regression and linear SVM learn a linear decision boundary, but they differ in objective functions and outputs. Logistic Regression minimizes the log-loss (cross-entropy) and naturally produces calibrated probabilities, while linear SVM minimizes the hinge loss and focuses on maximizing the margin between classes. As a result, SVM often provides strong classification accuracy, but it does not output probabilities unless additional calibration is applied. Therefore, for this salary-threshold prediction problem where probability interpretation and threshold tuning are important, Logistic Regression is typically the better choice."
      ],
      "metadata": {
        "id": "jI_QBkU0e9oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Dataset and Advanced EDA"
      ],
      "metadata": {
        "id": "TC2puH6jZwz1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3k76DL9ZwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "# Replace 'my_folder/my_data.csv' with your file's actual path\n",
        "file_path = '/content/drive/MyDrive/'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Z0vknJ9SXw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0dv813QXm01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}